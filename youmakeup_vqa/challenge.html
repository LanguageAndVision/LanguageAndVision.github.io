
<!DOCTYPE HTML>
<html>
<head>
<title>YouMakeup Video Question Answering Challenge</title>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
<link rel="stylesheet" href="static/css/main.css" />
</head>
<body>

<!-- Header -->
<header id="header">
  <div class="inner">
    <a href="index.html" class="logo">YouMakeup</a>
    <nav id="nav">
      <a href="index.html">Home</a>
      <a href="challenge.html">Challenge</a>
      <a href="dataset.html">Dataset</a>
      <a href="people.html">People</a>
    </nav>
  </div>
</header>
<a href="#menu" class="navPanelToggle"><span class="fa fa-bars"></span></a>


<!-- Main -->
<section id="main" >
  <div class="inner">
    <header class="major special">
      <h2 style='color:#0f4c8a' >Task Description</h2>
    </header>
    <br>
      <h3 class='subtitle' ><li>Image Ordering Sub-Challenge</li></h3>
        <p>The task is to sort a set of facial images from a video into the correct order according to given step descriptions. 
        The goal of this task is to understand the changes that a given action described in natural language will cause to a face object.
        The effects of action descriptions on facial appearances can vary greatly, depending not only on the text description, but also on the previous state of the facial appearance.
        Some actions may bring obvious facial changes, such as "apply red lipsticks on the lips", while some actions only cause slight differences, such as "apply foundation on the face with brush", which can be better detected if the previous appearance status is known.
        Therefore, fine-grained multimodal analysis on visual faces and textual actions is necessary to tackle this task.</p>
        <span class="image fit"><img src="static/img/image_ordering.png" alt="" /></span>
      <h3 class='subtitle' ><li>Step Ordering Sub-Challenge</li></h3>
        <p>The task is to sort a set of action descriptions into the right order that these actions are performed in the video. 
        It aims at evaluating models' abilities in cross-modal semantic alignments between visual and texts.
        Compared with previous video-text cross-modal localization, the novelty of this task has three aspects.
        Firstly, different actions share similar background contexts, thus it requires the model to specifically focus on actions and action-related objects instead of correlated but irrelevant contexts.
        Secondly, since different actions can be very similar in visual appearance, the task demands fine-grained discrimination in particular.
        Finally, our task goes beyond mere single text to single video localization and requires long-term temporal action reasoning and textual understanding.</p>
        <span class="image fit"><img src="static/img/step_ordering.png" alt="" /></span>
        <br>
    </div>
</section>


<section id='guideline'>
    <div class="inner">
        <header class="major special">
            <h2 style='color:#0f4c8a' >Challenge Guidelines</h2>
        </header>
        <br>
        <h3 class='subtitle' ><li>Dataset Download</li></h3>
        <p>Please refer to the details at the <a href="dataset.html">Dataset</a> page.</p>
        <h3 class='subtitle' ><li>Submission</li></h3>
        <p>The challenge is hosted at the CodaLab. Please go to the <a href="https://competitions.codalab.org/competitions/24108">challenge</a> page to submit your results.</p>
        <h3 class='subtitle' ><li>Evaluation Metrics</li></h3>
        <p>The two tasks are evaluated by accuracy of multi-choice selection.</p>
        <h3 class='subtitle' ><li>Requirements</li></h3>
        <p>1. Participants should stick to the definition of training, validation and test partition in order to have a fair comparison of different approaches.</p>
        <p>2. The Challenge is a team-based contest. Each team can have one or more members, and an individual cannot be a member of multiple teams. </p>
        <br>
        <p>3. Each team can submit at most two trials a day for each sub-challenge on test partition.</p>
        <p>4. At the end of the Challenge, all teams will be ranked based on the evaluation described above. The top teams will receive award certificates.</p>
        <br><br>
        <h3 class='subtitle' ><li>Baseline Paper</li></h3>
        <p>The paper introducing the YouMakeup VQA Challenge baseline can be viewed <a href="https://arxiv.org/abs/2004.05573">here</a>.</p>
        <p>The baseline codes and models are released <a href="https://github.com/AIM3-RUC/Youmakeup_Baseline">here</a>.</p>
        <br>
        <div class="inner">
    <h5>Please cite our baseline paper as below if you find it useful.</h5>
    <pre><code>
@misc{chen2020youmakeup,
    title={YouMakeup VQA Challenge: Towards Fine-grained Action Understanding in Domain-Specific Videos},
    author={Shizhe Chen and Weiying Wang and Ludan Ruan and Linli Yao and Qin Jin},
    year={2020},
    eprint={2004.05573},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
  </code></pre>
 </div>
    </div>
</section>
<br><br><br>


<!-- Scripts -->
<script src="/static/js/jquery.min.js"></script>
<script src="/static/js/skel.min.js"></script>
<script src="/static/js/util.js"></script>
<script src="/static/js/main.js"></script>


</body>
</html>